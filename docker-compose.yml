version: '3'
networks:
    elasticnet:
        driver: bridge
        ipam:
            driver: default
            config: [{subnet: 10.10.10.0/16}]
services:

    containerelasticsearch:
        image: 'docker.elastic.co/elasticsearch/elasticsearch:6.3.2'
        privileged: true
        hostname: containerelasticsearch
        container_name: containerelasticsearch
        restart: on-failure
        
        networks:
            - elasticnet
        ports:
            - '9202:9200'
            - '9357:9300'
        environment:
            - ES_CONNECT_RETRY=1200
            - MAX_MAP_COUNT=262144
            - bootstrap.memory_lock=true
            - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
        ulimits:
            memlock:
                soft: -1
                hard: -1
        volumes:
            - './ElasticSearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml'
            - './ElasticSearch/jvm.options:/usr/share/elasticsearch/config/jvm.options'
            
    containerkibana:
        image: 'docker.elastic.co/kibana/kibana:6.3.2'
        privileged: true
        hostname: containerkibana
        container_name: containerkibana
        depends_on:
            - containerelasticsearch
        networks:
            - elasticnet
        ports:
            - '5607:5601'
        volumes:
            - './Kibana/kibana.yml:/usr/share/kibana/config/kibana.yml'
    
    containerlogstash:
        image: 'docker.elastic.co/logstash/logstash:6.3.2'
        privileged: true
        hostname: containerlogstash
        container_name: containerlogstash
        depends_on:
            - containerelasticsearch
        restart: on-failure
        networks:
            - elasticnet
        ports:
            - '5049:5044'
        volumes:
            - './Logstash/Pipelines/:/usr/share/logstash/pipeline/'
            - './Logstash/Pipelines/:/etc/logstash/conf.d/'

    #elk:
    #    image: 'sebp/elk:latest'
    #    privileged: true
    #    hostname: elk
    #    container_name: elk
    #    restart: on-failure
    #    networks:
    #        - elasticnet
    #    ports:
    #        - '9200:9200'
    #        - '9300:9300'
    #        - '5601:5601'
    #        - '5044:5044'
    #    volumes:
    #        - './Logstash/Pipelines/:/usr/share/logstash/pipeline/'
    #        - './Logstash/Pipelines/:/etc/logstash/conf.d/'
        
    
    
    container_filebeat_spark_master:
        container_name: container_filebeat_spark_master
        hostname: container_filebeat_spark_master
        user: root
        image: "docker.elastic.co/beats/filebeat:6.5.4"
        volumes:
            - './Beats/Filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml'
            - './Beats/Filebeat/modules.d/:/usr/share/filebeat/modules.d/'
            - './Logs/Spark_Master/:/var/log/spark_master/'
        networks:
            - elasticnet
        #command:  'filebeat setup --template -E output.logstash.enabled=false  -E beat.name=master -E ''output.elasticsearch.hosts=["containerelasticsearch:9200"]'' && filebeat -e -c /etc/filebeat/filebeat.yml'
        restart: on-failure
        depends_on:
            - containerelasticsearch
            - containerlogstash
        environment:
            - output.elasticsearch.hosts= ["containerelasticsearch:9200"]
            - setup.kibana.host = 'containerkibana:5601'
            - beat.name= 'master'
            
    container_filebeat_spark_worker1:
        container_name: container_filebeat_spark_worker1
        hostname: container_filebeat_spark_worker1
        user: root
        image: "docker.elastic.co/beats/filebeat:6.5.4"
        volumes:
            - './Beats/Filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml'
            - './Beats/Filebeat/modules.d/:/usr/share/filebeat/modules.d/'
            - './Logs/Spark_Worker1/:/var/log/spark_worker1/'
        networks:
            - elasticnet
        #command:  'filebeat setup --template -E output.logstash.enabled=false  -E beat.name=wrk1 -E ''output.elasticsearch.hosts=["elk:9200"]'' && filebeat -e -c /etc/filebeat/filebeat.yml'
        restart: on-failure
        depends_on:
            - containerelasticsearch
            - containerlogstash
        environment:
            - output.elasticsearch.hosts= ["containerelasticsearch:9200"]
            - setup.kibana.host = 'containerkibana:5601'
            - beat.name= 'wrk1'
            
    container_filebeat_spark_worker2:
        container_name: container_filebeat_spark_worker2
        hostname: container_filebeat_spark_worker2
        user: root
        image: "docker.elastic.co/beats/filebeat:6.5.4"
        volumes:
            - './Beats/Filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml'
            - './Beats/Filebeat/modules.d/:/usr/share/filebeat/modules.d/'
            - './Logs/Spark_WorkerZ/:/var/log/spark_workerZ/'
        networks:
            - elasticnet
        #command: 'filebeat setup --template -E output.logstash.enabled=false  -E beat.name=wk2 -E ''output.elasticsearch.hosts=["elk:9200"]'' && filebeat -e -c /etc/filebeat/filebeat.yml'
        restart: on-failure
        depends_on:
            - containerelasticsearch
            - containerlogstash
        environment:
            - output.elasticsearch.hosts= ["containerelasticsearch:9200"]
            - setup.kibana.host = 'containerkibana:5601'
            - beat.name= 'wk2'
    
    container_spark_master:
        image: bde2020/spark-master:2.4.0-hadoop2.7
        privileged: true
        container_name: container_spark_master
        hostname: container_spark_master
        networks:
            - elasticnet
        ports:
            - '8087:8080'
            - '7078:7077'
        environment:
            - INIT_DAEMON_STEP=setup_spark
        volumes:
            - './Logs/Spark_Master/:/spark/logs/'
        restart: on-failure

            
    container_spark_worker_1:
        image: bde2020/spark-worker:2.4.0-hadoop2.7
        container_name: container_spark_worker_1
        hostname: container_spark_worker_1
        networks:
            - elasticnet
        depends_on:
            - container_spark_master
        ports:
            - '8088:8081'
        environment:
            - 'SPARK_MASTER=spark://spark_master:7077'
        volumes:
            - './Logs/Spark_Worker1/:/spark/logs/'
        restart: on-failure
        
    container_spark_worker_2:
        image: bde2020/spark-worker:2.4.0-hadoop2.7
        container_name: container_spark_worker_2
        hostname: container_spark_worker_2
        networks:
            - elasticnet
        depends_on:
            - container_spark_master
        ports:
            - '8089:8081'
        environment:
            - 'SPARK_MASTER=spark://spark_master:7077'
        volumes:
            - './Logs/Spark_Worker2/:/spark/logs/'
        restart: on-failure
        
