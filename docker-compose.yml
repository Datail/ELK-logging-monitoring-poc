version: '3'
networks:
    elasticnet:
        driver: bridge
        ipam:
            driver: default
            config: [{subnet: 10.10.10.0/16}]
services:

    container_elasticsearch:
        image: 'docker.elastic.co/elasticsearch/elasticsearch:6.3.2'
        privileged: true
        hostname: container_elasticsearch
        container_name: container_elasticsearch
        restart: on-failure
        networks:
            - elasticnet
        ports:
            - '9202:9200'
            - '9357:9300'
        environment:
            - ES_CONNECT_RETRY=1200
            - MAX_MAP_COUNT=262144 
        volumes:
            - './ElasticSearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml'
            
    container_kibana:
        image: 'docker.elastic.co/kibana/kibana:6.3.2'
        privileged: true
        hostname: container_kibana
        container_name: container_kibana
        restart: on-failure
        depends_on:
            - container_elasticsearch
        networks:
            - elasticnet
        ports:
            - '5607:5601'
        volumes:
            - './Kibana/kibana.yml:/usr/share/kibana/config/kibana.yml'
    
    container_logstash:
        image: 'docker.elastic.co/logstash/logstash:6.3.2'
        privileged: true
        hostname: container_logstash
        container_name: container_logstash
        depends_on:
            - container_elasticsearch
        restart: on-failure
        networks:
            - elasticnet
        ports:
            - '5049:5044'
        volumes:
            - './Logstash/Pipelines/:/usr/share/logstash/pipeline/'
            - './Logstash/Pipelines/:/etc/logstash/conf.d/'

    #elk:
    #    image: 'sebp/elk:latest'
    #    privileged: true
    #    hostname: elk
    #    container_name: elk
    #    restart: on-failure
    #    networks:
    #        - elasticnet
    #    ports:
    #        - '9200:9200'
    #        - '9300:9300'
    #        - '5601:5601'
    #        - '5044:5044'
    #    volumes:
    #        - './Logstash/Pipelines/:/usr/share/logstash/pipeline/'
    #        - './Logstash/Pipelines/:/etc/logstash/conf.d/'
        
    
    
    container_filebeat_spark_master:
        container_name: container_filebeat_spark_master
        hostname: container_filebeat_spark_master
        user: root
        image: "docker.elastic.co/beats/filebeat:6.5.4"
        volumes:
            - './Beats/Filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml'
            - './Beats/Filebeat/modules.d/:/usr/share/filebeat/modules.d/'
            - './Logs/Spark_Master/:/var/log/spark_master/'
        networks:
            - elasticnet
        #command:  'filebeat setup --template -E output.logstash.enabled=false  -E beat.name=master -E ''output.elasticsearch.hosts=["elk:9200"]'' && filebeat -e -c /etc/filebeat/filebeat.yml'
        restart: on-failure
        depends_on:
            - container_elasticsearch
            - container_logstash
        environment:
            - output.elasticsearch.hosts= ["container_elasticsearch:9200"]
            - setup.kibana.host = 'container_kibana:5601'
            - beat.name= 'master'
            
    container_filebeat_spark_worker1:
        container_name: container_filebeat_spark_worker1
        hostname: container_filebeat_spark_worker1
        user: root
        image: "docker.elastic.co/beats/filebeat:6.5.4"
        volumes:
            - './Beats/Filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml'
            - './Beats/Filebeat/modules.d/:/usr/share/filebeat/modules.d/'
            - './Logs/Spark_Worker1/:/var/log/spark_worker1/'
        networks:
            - elasticnet
        #command:  'filebeat setup --template -E output.logstash.enabled=false  -E beat.name=wrk1 -E ''output.elasticsearch.hosts=["elk:9200"]'' && filebeat -e -c /etc/filebeat/filebeat.yml'
        restart: on-failure
        depends_on:
            - container_elasticsearch
            - container_logstash
        environment:
            - output.elasticsearch.hosts= ["container_elasticsearch:9200"]
            - setup.kibana.host = 'container_kibana:5601'
            - beat.name= 'wrk1'
            
    container_filebeat_spark_worker2:
        container_name: container_filebeat_spark_worker2
        hostname: container_filebeat_spark_worker2
        user: root
        image: "docker.elastic.co/beats/filebeat:6.5.4"
        volumes:
            - './Beats/Filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml'
            - './Beats/Filebeat/modules.d/:/usr/share/filebeat/modules.d/'
            - './Logs/Spark_WorkerZ/:/var/log/spark_workerZ/'
        networks:
            - elasticnet
        #command: 'filebeat setup --template -E output.logstash.enabled=false  -E beat.name=wk2 -E ''output.elasticsearch.hosts=["elk:9200"]'' && filebeat -e -c /etc/filebeat/filebeat.yml'
        restart: on-failure
        depends_on:
            - container_elasticsearch
            - container_logstash
        environment:
            - output.elasticsearch.hosts= ["container_elasticsearch:9200"]
            - setup.kibana.host = 'container_kibana:5601'
            - beat.name= 'wk2'
    
    container_spark_master:
        image: bde2020/spark-master:2.4.0-hadoop2.7
        privileged: true
        container_name: container_spark_master
        hostname: container_spark_master
        networks:
            - elasticnet
        ports:
            - '8087:8080'
            - '7078:7077'
        environment:
            - INIT_DAEMON_STEP=setup_spark
        volumes:
            - './Logs/Spark_Master/:/spark/logs/'
        restart: on-failure

            
    container_spark_worker_1:
        image: bde2020/spark-worker:2.4.0-hadoop2.7
        container_name: container_spark_worker_1
        hostname: container_spark_worker_1
        networks:
            - elasticnet
        depends_on:
            - container_spark_master
        ports:
            - '8088:8081'
        environment:
            - 'SPARK_MASTER=spark://spark_master:7077'
        volumes:
            - './Logs/Spark_Worker1/:/spark/logs/'
        restart: on-failure
        
    container_spark_worker_2:
        image: bde2020/spark-worker:2.4.0-hadoop2.7
        container_name: container_spark_worker_2
        hostname: container_spark_worker_2
        networks:
            - elasticnet
        depends_on:
            - container_spark_master
        ports:
            - '8089:8081'
        environment:
            - 'SPARK_MASTER=spark://spark_master:7077'
        volumes:
            - './Logs/Spark_Worker2/:/spark/logs/'
        restart: on-failure
        
